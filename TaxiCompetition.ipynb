{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717bbecc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-23T04:36:01.640410Z",
     "iopub.status.busy": "2024-08-23T04:36:01.639973Z",
     "iopub.status.idle": "2024-08-23T04:37:06.513562Z",
     "shell.execute_reply": "2024-08-23T04:37:06.512029Z"
    },
    "papermill": {
     "duration": 64.891014,
     "end_time": "2024-08-23T04:37:06.525677",
     "exception": false,
     "start_time": "2024-08-23T04:36:01.634663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812366 sha256=c347fa134009ece1ec3c7032395d874e0940dfbfb74dfcf644dbf03c55179e2e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.2\r\n",
      "Requirement already satisfied: geopy in /opt/conda/lib/python3.10/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /opt/conda/lib/python3.10/site-packages (from geopy) (2.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/new-york-city-taxi-fare-prediction/sample_submission.csv\n",
      "/kaggle/input/new-york-city-taxi-fare-prediction/GCP-Coupons-Instructions.rtf\n",
      "/kaggle/input/new-york-city-taxi-fare-prediction/train.csv\n",
      "/kaggle/input/new-york-city-taxi-fare-prediction/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "!pip install pyspark\n",
    "!pip install geopy\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, mean, stddev, col, abs, date_format, hour, minute, when, lit, monotonically_increasing_id, round\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "def compute_distance_udf(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "    coord1 = (pickup_latitude, pickup_longitude)\n",
    "    coord2 = (dropoff_latitude, dropoff_longitude)\n",
    "    return geodesic(coord1, coord2).kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3ea12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:37:06.547923Z",
     "iopub.status.busy": "2024-08-23T04:37:06.547334Z",
     "iopub.status.idle": "2024-08-23T04:37:06.572147Z",
     "shell.execute_reply": "2024-08-23T04:37:06.571032Z"
    },
    "papermill": {
     "duration": 0.038994,
     "end_time": "2024-08-23T04:37:06.574695",
     "exception": false,
     "start_time": "2024-08-23T04:37:06.535701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainTaxiCompetition:\n",
    "    \n",
    "    def __init__(self, frac=1, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"TaxiCompetition\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        self.sc = self.spark.sparkContext\n",
    "\n",
    "        self.sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "        self.data = self.spark.read.csv('/kaggle/input/new-york-city-taxi-fare-prediction/train.csv', header=True, inferSchema=True)\n",
    "        self.sample_data = self.data.sample(fraction=frac, seed=self.random_state)\n",
    "        \n",
    "    def data_cleaning(self):\n",
    "        self.sample_data = self.sample_data.filter((col('pickup_longitude') <= 180) & (col('pickup_longitude') >= -180) & (col('dropoff_longitude') <= 180) & (col('dropoff_longitude') >= -180) & (col('pickup_latitude') <= 90) & (col('pickup_latitude') >= -90) & (col('dropoff_latitude') <= 90) & (col('dropoff_latitude') >= -90))\n",
    "\n",
    "        self.data_wo_dup = self.sample_data.dropDuplicates()\n",
    "\n",
    "        self.mean = self.data_wo_dup.select(mean(\"fare_amount\")).collect()[0][0]\n",
    "        self.std = self.data_wo_dup.select(stddev(\"fare_amount\")).collect()[0][0]\n",
    "\n",
    "        self.data_wo_dup = self.data_wo_dup.withColumn(\"zscore\", \n",
    "                                      (col(\"fare_amount\") - self.mean) / self.std)\n",
    "\n",
    "        self.data_wo_dup_out = self.data_wo_dup.filter(col('zscore') < 3)\n",
    "        \n",
    "    def new_columns(self):\n",
    "        def compute_distance_udf(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "            coord1 = (pickup_latitude, pickup_longitude)\n",
    "            coord2 = (dropoff_latitude, dropoff_longitude)\n",
    "            return geodesic(coord1, coord2).kilometers\n",
    "        \n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn('long', abs(col('dropoff_longitude') - col('pickup_longitude')))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn('lat', abs(col('dropoff_latitude') - col('pickup_latitude')))\n",
    "\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn('weekday', date_format(col('pickup_datetime'), 'EEEE'))\n",
    "\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn('hour_minute', hour(col('pickup_datetime')) + (minute(col('pickup_datetime'))/60))\n",
    "        \n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Tuesday\", when(col(\"weekday\") == 'Tuesday', 1).otherwise(0))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Wednesday\", when(col(\"weekday\") == 'Wednesday', 1).otherwise(0))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Thursday\", when(col(\"weekday\") == 'Thursday', 1).otherwise(0))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Friday\", when(col(\"weekday\") == 'Friday', 1).otherwise(0))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Saturday\", when(col(\"weekday\") == 'Saturday', 1).otherwise(0))\n",
    "        self.data_wo_dup_out = self.data_wo_dup_out.withColumn(\"Sunday\", when(col(\"weekday\") == 'Sunday', 1).otherwise(0))\n",
    "\n",
    "        # Define UDF\n",
    "        distance_udf = udf(compute_distance_udf, DoubleType())\n",
    "        self.data = self.data_wo_dup_out.withColumn('distance_km', distance_udf(\n",
    "        col('pickup_latitude'),\n",
    "        col('pickup_longitude'),\n",
    "        col('dropoff_latitude'),\n",
    "        col('dropoff_longitude')\n",
    "        ))\n",
    "        \n",
    "    def train_model(self):\n",
    "\n",
    "        self.data_selected = self.data.select(\n",
    "            'fare_amount', 'passenger_count', 'long', 'lat', 'hour_minute', 'distance_km',\n",
    "            'Friday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'\n",
    "        )\n",
    "\n",
    "        self.X_train, self.X_test = self.data_selected.randomSplit([0.8, 0.2], seed=self.random_state)\n",
    "\n",
    "        self.assembler = VectorAssembler(\n",
    "            inputCols=[\n",
    "                'passenger_count', 'long', 'lat', 'hour_minute', 'distance_km',\n",
    "                'Friday', 'Saturday', 'Sunday',\n",
    "                'Thursday', 'Tuesday', 'Wednesday'\n",
    "            ],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        self.train_assembled_df = self.assembler.transform(self.X_train)\n",
    "        self.test_assembled_df = self.assembler.transform(self.X_test)\n",
    "\n",
    "        self.rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"fare_amount\")\n",
    "        self.rf_model = self.rf.fit(self.train_assembled_df)\n",
    "        self.rf_predictions = self.rf_model.transform(self.test_assembled_df)\n",
    "\n",
    "        self.evaluator = RegressionEvaluator(\n",
    "            labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    "        )\n",
    "        self.rmse = self.evaluator.evaluate(self.rf_predictions)\n",
    "        print(f\"Root Mean Squared Error (RMSE): {self.rmse}\")\n",
    "            \n",
    "    def run(self):\n",
    "        self.data_cleaning()\n",
    "        self.new_columns()\n",
    "        self.train_model()\n",
    "        \n",
    "        return self.rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b829beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:37:06.597045Z",
     "iopub.status.busy": "2024-08-23T04:37:06.596611Z",
     "iopub.status.idle": "2024-08-23T04:37:06.613811Z",
     "shell.execute_reply": "2024-08-23T04:37:06.612493Z"
    },
    "papermill": {
     "duration": 0.031525,
     "end_time": "2024-08-23T04:37:06.616390",
     "exception": false,
     "start_time": "2024-08-23T04:37:06.584865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestTaxiCompetition:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"TaxiCompetition\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        self.sc = self.spark.sparkContext\n",
    "\n",
    "        self.sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "        self.data = self.spark.read.csv('/kaggle/input/new-york-city-taxi-fare-prediction/test.csv', header=True, inferSchema=True)\n",
    "        \n",
    "    def new_columns(self):\n",
    "        def compute_distance_udf(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "            coord1 = (pickup_latitude, pickup_longitude)\n",
    "            coord2 = (dropoff_latitude, dropoff_longitude)\n",
    "            return geodesic(coord1, coord2).kilometers\n",
    "        \n",
    "        self.data = self.data.withColumn('long', abs(col('dropoff_longitude') - col('pickup_longitude')))\n",
    "        self.data = self.data.withColumn('lat', abs(col('dropoff_latitude') - col('pickup_latitude')))\n",
    "\n",
    "        self.data = self.data.withColumn('weekday', date_format(col('pickup_datetime'), 'EEEE'))\n",
    "\n",
    "        self.data = self.data.withColumn('hour_minute', hour(col('pickup_datetime')) + (minute(col('pickup_datetime'))/60))\n",
    "        \n",
    "        self.data = self.data.withColumn(\"Tuesday\", when(col(\"weekday\") == 'Tuesday', 1).otherwise(0))\n",
    "        self.data = self.data.withColumn(\"Wednesday\", when(col(\"weekday\") == 'Wednesday', 1).otherwise(0))\n",
    "        self.data = self.data.withColumn(\"Thursday\", when(col(\"weekday\") == 'Thursday', 1).otherwise(0))\n",
    "        self.data = self.data.withColumn(\"Friday\", when(col(\"weekday\") == 'Friday', 1).otherwise(0))\n",
    "        self.data = self.data.withColumn(\"Saturday\", when(col(\"weekday\") == 'Saturday', 1).otherwise(0))\n",
    "        self.data = self.data.withColumn(\"Sunday\", when(col(\"weekday\") == 'Sunday', 1).otherwise(0))\n",
    "\n",
    "        # Define UDF\n",
    "        distance_udf = udf(compute_distance_udf, DoubleType())\n",
    "        self.data = self.data.withColumn('distance_km', distance_udf(\n",
    "        col('pickup_latitude'),\n",
    "        col('pickup_longitude'),\n",
    "        col('dropoff_latitude'),\n",
    "        col('dropoff_longitude')\n",
    "        ))\n",
    "        \n",
    "    def test_model(self):\n",
    "\n",
    "        self.data_selected = self.data.select(\n",
    "            'passenger_count', 'long', 'lat', 'hour_minute', 'distance_km',\n",
    "            'Friday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'\n",
    "        )\n",
    "\n",
    "        self.assembler = VectorAssembler(\n",
    "            inputCols=[\n",
    "                'passenger_count', 'long', 'lat', 'hour_minute', 'distance_km',\n",
    "                'Friday', 'Saturday', 'Sunday',\n",
    "                'Thursday', 'Tuesday', 'Wednesday'\n",
    "            ],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        \n",
    "        self.assembled_df = self.assembler.transform(self.data_selected)\n",
    "\n",
    "        self.predictions = self.model.transform(self.assembled_df)\n",
    "        \n",
    "    def submission(self):\n",
    "        \n",
    "        self.data_key = self.data.select('key')\n",
    "        self.data.predictions = self.predictions.select('prediction')\n",
    "        \n",
    "        self.data_key = self.data_key.withColumn(\"id\", monotonically_increasing_id())\n",
    "        self.data.predictions = self.data.predictions.withColumn(\"id\", monotonically_increasing_id())\n",
    "        \n",
    "        self.submission = self.data_key.join(self.data.predictions, on=\"id\").drop(\"id\")\n",
    "        \n",
    "        self.submission = self.submission.withColumn(\"fare_amount\", round(col(\"prediction\"), 2)).drop('prediction')\n",
    "        \n",
    "        self.submission.withColumn(\"key\", date_format(\"key\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            \n",
    "    def run(self):\n",
    "\n",
    "        self.new_columns()\n",
    "        self.test_model()\n",
    "        self.submission()\n",
    "        \n",
    "        return self.submission\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77742494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:37:06.638289Z",
     "iopub.status.busy": "2024-08-23T04:37:06.637902Z",
     "iopub.status.idle": "2024-08-23T04:50:47.457440Z",
     "shell.execute_reply": "2024-08-23T04:50:47.455504Z"
    },
    "papermill": {
     "duration": 820.834563,
     "end_time": "2024-08-23T04:50:47.461052",
     "exception": false,
     "start_time": "2024-08-23T04:37:06.626489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/23 04:37:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.2.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 39:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 3.3376992574151116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = TrainTaxiCompetition(frac=0.001).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b885e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:50:47.532859Z",
     "iopub.status.busy": "2024-08-23T04:50:47.531765Z",
     "iopub.status.idle": "2024-08-23T04:50:48.505113Z",
     "shell.execute_reply": "2024-08-23T04:50:48.503567Z"
    },
    "papermill": {
     "duration": 1.013902,
     "end_time": "2024-08-23T04:50:48.509192",
     "exception": false,
     "start_time": "2024-08-23T04:50:47.495290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = TestTaxiCompetition(model).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158d28c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:50:48.567658Z",
     "iopub.status.busy": "2024-08-23T04:50:48.566328Z",
     "iopub.status.idle": "2024-08-23T04:50:51.772663Z",
     "shell.execute_reply": "2024-08-23T04:50:51.770550Z"
    },
    "papermill": {
     "duration": 3.234217,
     "end_time": "2024-08-23T04:50:51.775854",
     "exception": false,
     "start_time": "2024-08-23T04:50:48.541637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|                key|fare_amount|\n",
      "+-------------------+-----------+\n",
      "|2015-01-27 13:08:24|        8.4|\n",
      "|2015-01-27 13:08:24|       8.81|\n",
      "|2011-10-08 11:53:44|       6.31|\n",
      "|2012-12-01 21:12:12|        7.6|\n",
      "|2012-12-01 21:12:12|      15.02|\n",
      "|2012-12-01 21:12:12|       9.96|\n",
      "|2011-10-06 12:10:20|       6.33|\n",
      "|2011-10-06 12:10:20|      26.96|\n",
      "|2011-10-06 12:10:20|      12.16|\n",
      "|2014-02-18 15:22:20|       6.31|\n",
      "|2014-02-18 15:22:20|       8.31|\n",
      "|2014-02-18 15:22:20|      13.98|\n",
      "|2010-03-29 20:20:32|       6.31|\n",
      "|2010-03-29 20:20:32|       8.04|\n",
      "|2011-10-06 03:59:12|       8.52|\n",
      "|2011-10-06 03:59:12|      14.45|\n",
      "|2012-07-15 16:45:04|       6.32|\n",
      "|2012-07-15 16:45:04|      10.03|\n",
      "|2012-07-15 16:45:04|       6.32|\n",
      "|2012-07-15 16:45:04|       6.32|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75745b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T04:50:51.830877Z",
     "iopub.status.busy": "2024-08-23T04:50:51.830293Z",
     "iopub.status.idle": "2024-08-23T04:50:55.232789Z",
     "shell.execute_reply": "2024-08-23T04:50:55.231582Z"
    },
    "papermill": {
     "duration": 3.428349,
     "end_time": "2024-08-23T04:50:55.235332",
     "exception": false,
     "start_time": "2024-08-23T04:50:51.806983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "output.write.csv('submission.csv', header=True, mode='overwrite')\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6fcc",
   "metadata": {
    "papermill": {
     "duration": 0.032213,
     "end_time": "2024-08-23T04:50:55.289770",
     "exception": false,
     "start_time": "2024-08-23T04:50:55.257557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 61318,
     "sourceId": 10170,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 899.734017,
   "end_time": "2024-08-23T04:50:57.937260",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-23T04:35:58.203243",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
